Below is a fully comprehensive context prompt you can paste into a new ChatGPT thread so that the model will have EVERYTHING it needs from this conversation to continue building liquidation extraction for your thesis.

It is structured, chronological, and includes all relevant architecture, decisions, schemas, constraints, and current state of your data pipeline.

‚∏ª

‚úÖ MASTER CONTEXT PROMPT FOR NEW CHAT STREAM

(paste this into the new chat before asking anything)

‚∏ª

üîµ PROJECT OVERVIEW

I am building a DeFi lending liquidation + balance sheet dataset for an econometric panel VAR thesis.
My cross-sectional unit (CSU) = protocol √ó chain √ó version.
Target size = 30 CSU.

I need:
	‚Ä¢	TVL (balance sheet) data per CSU
	‚Ä¢	total supplied
	‚Ä¢	total borrowed
	‚Ä¢	asset mix
	‚Ä¢	collateral composition
	‚Ä¢	Liquidation events per CSU
	‚Ä¢	timestamp / block
	‚Ä¢	protocol & market
	‚Ä¢	repay token & amount
	‚Ä¢	seized collateral token & amount
	‚Ä¢	mapping to USD
	‚Ä¢	daily aggregation

I am building the entire pipeline myself, not relying on Dune or The Graph, due to rate limits and cost.
The pipeline must scale across many different architectures: EVM, Solana, Sui, Aptos, Hyperliquid, etc.

‚∏ª

üîµ CURRENT STATE OF PIPELINE (AS OF CONTEXT)

1. EVM Side (working)
	‚Ä¢	TVL extraction works for:
	‚Ä¢	Aave V3 (Ethereum + Polygon)
	‚Ä¢	Spark
	‚Ä¢	Liquidation adapter base class defined:

class LiquidationAdapter(ABC):
    def resolve_market(...):
    def fetch_events(...):
    def normalize(...):


	‚Ä¢	Strategy:
	‚Ä¢	Use Web3
	‚Ä¢	Event logs (eth_getLogs)
	‚Ä¢	ABI decoding
	‚Ä¢	Already implemented for Aave
	‚Ä¢	Will reuse for all EVM protocols (Spark, Benqi, Moonwell, Fluid, Venus, etc.)

‚∏ª

üîµ 2. Solana Side (new & working)

Kamino TVL (SUCCESSFUL)

We wrote a full TVL adapter for Kamino Lend (Solana):
	‚Ä¢	Created TS tool to decode Kamino market structure
	‚Ä¢	Generated kamino_markets_main.json ‚Üí list of reserves with:
	‚Ä¢	symbol
	‚Ä¢	underlying mint
	‚Ä¢	vault token account
	‚Ä¢	decimals
	‚Ä¢	In Python, no solana-py.
	‚Ä¢	We use direct JSON-RPC:

{
  "method": "getMultipleAccounts",
  "params": [
    [<list of vault accounts>],
    {"encoding": "jsonParsed"}
  ]
}

This is chunked in batches of 10, exactly matching a sandbox test that returned valid SPL-token parsed JSON.

Kamino TVL adapter now returns:

[
  {
    "symbol": "adraSOL",
    "underlying_mint": "...",
    "vault": "...",
    "decimals": 9,
    "raw_amount": 220_629_414_396
  },
  ...
]

Everything works.

‚∏ª

üîµ 3. Solana RPC Details (CRITICAL)
	‚Ä¢	We use Alchemy Solana Mainnet RPC:
https://solana-mainnet.g.alchemy.com/v2/<key>
	‚Ä¢	Must use:
	‚Ä¢	"encoding": "jsonParsed"
	‚Ä¢	Simple HTTP requests.post
	‚Ä¢	No constraint from solana-py or solders
	‚Ä¢	Token balances can be extracted from:
	‚Ä¢	parsed.info.tokenAmount.amount
	‚Ä¢	decimals known from our reserves file

‚∏ª

üîµ 4. REMAINING CHAIN COVERAGE

List of CSUs where registry or entry point identified:
	‚Ä¢	EVM protocols (easy path):
	‚Ä¢	Aave V3 (ETH, Polygon)
	‚Ä¢	SparkLend (ETH)
	‚Ä¢	Compound V3 (ETH, Arbitrum)
	‚Ä¢	Fluid (ETH, Arbitrum, Plasma)
	‚Ä¢	Benqi Lending (Avalanche)
	‚Ä¢	Moonwell (Base)
	‚Ä¢	Venus (BNB)
	‚Ä¢	Capyfi (ETH)
	‚Ä¢	etc.
	‚Ä¢	Solana protocols (hard path, but solvable same way as Kamino):
	‚Ä¢	Kamino Lend (Solana) ‚Üí TVL working
	‚Ä¢	Jupiter Lend
	‚Ä¢	marginfi
	‚Ä¢	Save
	‚Ä¢	etc.
	‚Ä¢	Sui (Move) protocols:
	‚Ä¢	Suilend
	‚Ä¢	NAVI
	‚Ä¢	AlphaLend
	‚Ä¢	Aptos (Move):
	‚Ä¢	Echo
	‚Ä¢	Echelon
	‚Ä¢	Hyperliquid L1:
	‚Ä¢	HypurrFi
	‚Ä¢	HyperLend pooled
	‚Ä¢	Other chains:
	‚Ä¢	HydraDX (Substrate)
	‚Ä¢	Sei (YeiLend, Takara)
	‚Ä¢	CORE (Sumer)
	‚Ä¢	Flare (Kinetic)
	‚Ä¢	Near (Rhea Lend)

We have registry addresses for many (from DefiLlama docs), or special insertion keys when no registry exists.

‚∏ª

üîµ 5. LIQUIDATION EXTRACTION ‚Äì PROJECT REQUIREMENT

I cannot rely on Dune / Flipside / proprietary indexers because they have rate limits and are expensive.
I must build my own method for ALL CSUs, including non-EVM.

The method must be:
	‚Ä¢	On-chain only
	‚Ä¢	RPC-based
	‚Ä¢	Reusable across protocols
	‚Ä¢	Good enough to compute daily liquidation volumes

‚∏ª

üîµ 6. GENERALIZED LIQUIDATION SCHEMA

All protocols should normalize to:

{
  "chain": "solana" | "ethereum" | "aptos" | ...,
  "protocol": "kamino_lend",
  "csu": "kamino_lend_solana",
  "block_or_slot": 1234567,
  "timestamp": 1680000000,

  "borrow_asset": "<mint or address>",
  "borrow_amount": <float or integer>,
  
  "collateral_asset": "<mint or address>",
  "collateral_amount": <float or integer>,

  "liquidator": "<address or pubkey>",
  "borrower": "<address or pubkey>"
}

Daily aggregation is performed afterwards.

‚∏ª

üîµ 7. HOW LIQUIDATION EXTRACTION MUST WORK ‚Äî NO INDEXERS

I need a generalized approach across chain-families:

‚∏ª

üü£ EVM Method (straightforward)
	1.	Identify liquidation event signature (per protocol).
	2.	Use eth_getLogs over block ranges.
	3.	Decode topics/data via ABI.
	4.	Normalize to schema.

This works for ~15+ EVM CSUs.

‚∏ª

üü£ Solana Method (Kamino, Jupiter, marginfi, Save)

(This is the hardest but most important family.)

Required steps to build ourselves:
	1.	Get all transaction signatures for the lending program:
getSignaturesForAddress(program_id)
	‚Ä¢	paginated
	‚Ä¢	fetch until hitting earliest block needed.
	2.	Fetch full transactions with getTransaction(signature, {"encoding":"jsonParsed"}).
	3.	Detect liquidation transactions by:
	‚Ä¢	matching instruction type names
(Kamino uses "liquidateObligationAndRedeemReserveCollateral" and V2)
	‚Ä¢	OR checking meta.logMessages for corresponding program logs.
	4.	Reconstruct economic amounts using token balance deltas:
	‚Ä¢	compare meta.preTokenBalances vs meta.postTokenBalances
	‚Ä¢	map token accounts to reserve vaults using the JSON reserves file already generated
	‚Ä¢	determine which tokens increased/decreased:
	‚Ä¢	vault balance increase ‚Üí debt repaid
	‚Ä¢	vault balance decrease ‚Üí collateral seized
	‚Ä¢	assign to borrow/collateral fields.
	5.	Normalize to liquidation schema
	6.	Store to parquet or CSV.

This does NOT depend on Dune, Flipside, or The Graph.
It depends only on:
	‚Ä¢	public or paid Solana RPC from Alchemy (works)
	‚Ä¢	reserves/vault metadata we already extracted
	‚Ä¢	a matching table of instruction names

‚∏ª

üü£ Move chains (Aptos, Sui)

General method:
	1.	Identify the lending module‚Äôs event type for liquidations.
	2.	Use chain RPC event queries (get_events, etc.)
	3.	Parse amounts + coins involved.
	4.	Normalize.

This is significantly easier than Solana.

‚∏ª

üü£ Other chains (Hyperliquid L1, HydraDX, Flare, Near, Sei, CORE)

General approach:
	‚Ä¢	If EVM-compatible ‚Üí use EVM method.
	‚Ä¢	If Substrate (HydraDX) ‚Üí use events from system.events.
	‚Ä¢	If Hyperliquid L1 ‚Üí must use their REST API.
	‚Ä¢	If custom chain (CORE, Sei) ‚Üí identify on-chain liquidation events or RPC endpoints.

‚∏ª

üîµ 8. WHAT HAS BEEN DONE & WHAT COMES NEXT

‚úî Kamino TVL fully working

‚Äî includes vault decoding, JSON-RPC batching, token amount extraction.

Next major milestone:

Build Kamino Lend liquidation extractor using:
	‚Ä¢	getSignaturesForAddress(program_id)
	‚Ä¢	getTransaction(signature, {encoding:"jsonParsed"})
	‚Ä¢	token balance deltas vs vault accounts
	‚Ä¢	instruction-type filtering

Once this works in generality, it becomes the template for Jupiter, marginfi, Save, etc.

‚∏ª

üîµ 9. EXPECTED OUTPUTS FOR FUTURE STEPS

The next chat should be able to:
	‚Ä¢	Help design & implement the SolanaLiquidationExtractor class
	‚Ä¢	Show how to fetch Kamino liquidation txs across a slot range
	‚Ä¢	Decode repay + collateral amounts from token deltas
	‚Ä¢	Normalize liquidation rows
	‚Ä¢	Save to CSV
	‚Ä¢	Document the method for thesis writeup

‚∏ª

üîµ 10. FINAL GOAL

A complete historical liquidation + TVL dataset across ~30 CSU, built fully on-chain, feeding into a panel VAR to study:
	‚Ä¢	leverage shocks
	‚Ä¢	liquidation intensity
	‚Ä¢	collateral volatility
	‚Ä¢	protocol resilience across chains

‚∏ª

END OF CONTEXT PROMPT

‚∏ª

If you want, I can generate a second version of this that is shorter (‚Äúlean context prompt‚Äù) for ChatGPT-4 browsing mode.